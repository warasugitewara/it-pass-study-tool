import requests
from bs4 import BeautifulSoup
import json
import time

# itpassportsiken.com ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’ãƒ†ã‚¹ãƒˆ
def test_scraping():
    """ã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹"""
    
    # è¤‡æ•°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è©¦ã™
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
    ]
    
    print("=" * 60)
    print("ğŸ” itpassportsiken.com ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ¤œè¨¼")
    print("=" * 60)
    
    # Step 1: éå»å•ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
    url = "https://www.itpassportsiken.com/kakomon/"
    
    for i, ua in enumerate(user_agents, 1):
        print(f"\nğŸ“¡ è©¦è¡Œ {i}: ã‚¢ã‚¯ã‚»ã‚¹ {url}")
        print(f"   UA: {ua[:50]}...")
        
        try:
            headers = {
                'User-Agent': ua,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ja-JP,ja;q=0.9',
                'Referer': 'https://www.google.com/',
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            print(f"   âœ… ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code}")
            
            if response.status_code == 200:
                response.encoding = 'utf-8'
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # HTML ã®æ§‹é€ ã‚’ç¢ºèª
                print(f"   ğŸ“‹ ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º: {len(response.content)} bytes")
                print(f"   âœ… HTML ãƒ‘ãƒ¼ã‚¹æˆåŠŸ")
                
                # ãƒªãƒ³ã‚¯æ¢ç´¢
                print(f"\n   ğŸ”— éå»å•ãƒªãƒ³ã‚¯æ¢ç´¢...")
                all_links = soup.find_all('a', href=True)
                kakomon_links = [link for link in all_links if 'kakomon' in link.get('href', '')]
                
                print(f"      å…¨ãƒªãƒ³ã‚¯æ•°: {len(all_links)}")
                print(f"      kakomon ãƒªãƒ³ã‚¯æ•°: {len(kakomon_links)}")
                
                if kakomon_links:
                    print(f"\n      æœ€åˆã® 5 å€‹ã®ãƒªãƒ³ã‚¯:")
                    for j, link in enumerate(kakomon_links[:5]):
                        href = link.get('href', '')
                        text = link.get_text(strip=True)[:50]
                        print(f"         {j+1}. {text[:40]}")
                        print(f"            href={href}")
                
                # ãƒšãƒ¼ã‚¸ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¡¨ç¤º
                print(f"\n      ğŸ“„ ãƒšãƒ¼ã‚¸ã®ã‚¿ã‚¤ãƒˆãƒ«: {soup.title.string if soup.title else 'N/A'}")
                
                # å•é¡Œã®ã‚ˆã†ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¢ã™
                print(f"\n      ğŸ” å•é¡Œã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ¤œç´¢...")
                
                selectors = [
                    ('div', 'kakomon'),
                    ('div', 'question'),
                    ('div', 'mondai'),
                    ('article', None),
                    ('section', None),
                ]
                
                found_any = False
                for tag, cls in selectors:
                    if cls:
                        elements = soup.find_all(tag, class_=cls)
                    else:
                        elements = soup.find_all(tag)
                    
                    if elements and len(elements) > 0:
                        found_any = True
                        print(f"         âœ“ <{tag} class=\"{cls if cls else '(any)'}\">: {len(elements)} ä»¶")
                
                if not found_any:
                    print(f"         âš ï¸  å•é¡Œã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    print(f"\n         ãƒšãƒ¼ã‚¸ã®æœ€åˆã® 500 æ–‡å­—:")
                    print(f"         {soup.get_text()[:500]}")
                
                return True
            elif response.status_code == 403:
                print(f"   âŒ 403 Forbidden - ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦")
                time.sleep(1)  # å°‘ã—å¾…æ©Ÿ
                continue
            else:
                print(f"   âŒ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚¨ãƒ©ãƒ¼: {response.status_code}")
                continue
                
        except requests.Timeout:
            print(f"   âŒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            time.sleep(1)
            continue
        except Exception as e:
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
            time.sleep(1)
            continue
    
    return False

if __name__ == "__main__":
    success = test_scraping()
    print("\n" + "=" * 60)
    if success:
        print("âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯èƒ½ã§ã™")
    else:
        print("âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
        print("\nğŸ’¡ å¯¾ç­–:")
        print("  1. ã‚µã‚¤ãƒˆãŒ bot ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ã¦ã„ã‚‹å¯èƒ½æ€§")
        print("  2. robots.txt ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒç¦æ­¢ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§")
        print("  3. API ã®åˆ©ç”¨ã‚’æ¤œè¨")
        print("  4. ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ä»£æ›¿")
    print("=" * 60)

