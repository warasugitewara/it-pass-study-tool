import requests
from bs4 import BeautifulSoup
import json

def test_ipa_scraping():
    """IPAå…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’ãƒ†ã‚¹ãƒˆ"""
    
    url = "https://www3.jitec.ipa.go.jp/JitesCbt/html/openinfo/questions.html"
    
    print("=" * 70)
    print("ğŸ” IPAå…¬å¼ã‚µã‚¤ãƒˆ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ¤œè¨¼")
    print("=" * 70)
    print(f"\nğŸ“¡ URL: {url}")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ja-JP,ja;q=0.9',
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        print(f"\nâœ… ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code}")
        response.encoding = 'utf-8'
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            print(f"âœ… ãƒšãƒ¼ã‚¸è§£ææˆåŠŸ")
            print(f"   ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º: {len(response.content):,} bytes")
            
            # ã‚¿ã‚¤ãƒˆãƒ«ç¢ºèª
            if soup.title:
                print(f"   ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: {soup.title.string}")
            
            # ãƒªãƒ³ã‚¯æ¢ç´¢
            print(f"\nğŸ”— ãƒªãƒ³ã‚¯æ¢ç´¢...")
            all_links = soup.find_all('a', href=True)
            print(f"   å…¨ãƒªãƒ³ã‚¯æ•°: {len(all_links)}")
            
            # PDF/ãƒ‡ãƒ¼ã‚¿ãƒªãƒ³ã‚¯ã‚’æ¢ã™
            pdf_links = [link for link in all_links if '.pdf' in link.get('href', '').lower()]
            print(f"   PDFãƒªãƒ³ã‚¯: {len(pdf_links)}")
            
            if pdf_links:
                print(f"\n   æœ€åˆã®5ã¤ã®PDFãƒªãƒ³ã‚¯:")
                for i, link in enumerate(pdf_links[:5]):
                    href = link.get('href', '')
                    text = link.get_text(strip=True)[:50]
                    print(f"      {i+1}. {text}")
                    print(f"         {href[:70]}")
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«æ¢ç´¢
            print(f"\nğŸ“Š ãƒ†ãƒ¼ãƒ–ãƒ«æ¢ç´¢...")
            tables = soup.find_all('table')
            print(f"   ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(tables)}")
            
            for i, table in enumerate(tables[:3]):
                rows = table.find_all('tr')
                cols = table.find_all('td')
                print(f"      ãƒ†ãƒ¼ãƒ–ãƒ«{i+1}: {len(rows)} è¡Œ, {len(cols)} ã‚»ãƒ«")
                
                # æœ€åˆã®è¡Œã‚’è¡¨ç¤º
                if rows:
                    first_row_text = rows[0].get_text(strip=True)[:80]
                    print(f"         å†…å®¹: {first_row_text}")
            
            # å•é¡Œãƒªã‚¹ãƒˆã®æ¢ç´¢
            print(f"\nğŸ“ å•é¡Œã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¢ç´¢...")
            
            # div ã‚„ section ã§å•é¡Œã®ã‚ˆã†ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¢ã™
            content_divs = soup.find_all(['div', 'section', 'article'])
            print(f"   div/section/article: {len(content_divs)} ä»¶")
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦‹ã¦å•é¡Œã‚‰ã—ã„ã‚‚ã®ã‚’æ¢ã™
            all_text = soup.get_text()
            if 'éå»å•' in all_text:
                print(f"   âœ“ 'éå»å•' ã¨ã„ã†ãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
            if 'ITãƒ‘ã‚¹ãƒãƒ¼ãƒˆ' in all_text:
                print(f"   âœ“ 'ITãƒ‘ã‚¹ãƒãƒ¼ãƒˆ' ã¨ã„ã†ãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
            if 'å•é¡Œ' in all_text:
                print(f"   âœ“ 'å•é¡Œ' ã¨ã„ã†ãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
            
            # å¹´å·ã‚„å­£ç¯€ã‚’æ¢ã™
            seasons = ['æ˜¥', 'ç§‹', 'å†¬', 'å¤']
            years = ['2024', '2025', '2026']
            
            found_seasons = [s for s in seasons if s in all_text]
            found_years = [y for y in years if y in all_text]
            
            if found_seasons:
                print(f"   âœ“ å­£ç¯€æƒ…å ±: {', '.join(found_seasons)}")
            if found_years:
                print(f"   âœ“ å¹´å·æƒ…å ±: {', '.join(found_years)}")
            
            # HTMLã®æ¦‚è¦ã‚’è¡¨ç¤º
            print(f"\nğŸ“„ ãƒšãƒ¼ã‚¸ã®æœ€åˆã® 1000 æ–‡å­—:")
            print("   " + all_text[:1000].replace('\n', '\n   '))
            
            return True
        else:
            print(f"âŒ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚¨ãƒ©ãƒ¼: {response.status_code}")
            return False
    
    except requests.Timeout:
        print(f"âŒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
        return False
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_ipa_scraping()
    print("\n" + "=" * 70)
    if success:
        print("âœ… IPAå…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒå¯èƒ½ã§ã™ï¼")
    else:
        print("âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
    print("=" * 70)
